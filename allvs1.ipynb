{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11076094,"sourceType":"datasetVersion","datasetId":6903011}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T16:04:14.889981Z","iopub.execute_input":"2025-03-18T16:04:14.890305Z","iopub.status.idle":"2025-03-18T16:04:18.096655Z","shell.execute_reply.started":"2025-03-18T16:04:14.890277Z","shell.execute_reply":"2025-03-18T16:04:18.095523Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Folder where dataset is stored in Kaggle\nfolder = \"/kaggle/input/research\"\n\n# Folder to save results (Kaggle working directory)\nsave_folder = \"/kaggle/working\"\n\n# List all files in the dataset folder\nfiles = os.listdir(folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T16:12:14.467323Z","iopub.execute_input":"2025-03-18T16:12:14.467675Z","iopub.status.idle":"2025-03-18T16:12:14.480518Z","shell.execute_reply.started":"2025-03-18T16:12:14.467650Z","shell.execute_reply":"2025-03-18T16:12:14.478946Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom xgboost import XGBClassifier\n\nfor file_name in files:\n    print(f\"\\nProcessing file: {file_name} ...\")\n    \n    path = os.path.join(folder, file_name)\n    dataframe = pd.read_csv(path)\n    print(\"CSV file loaded successfully.\")\n\n    # Extract features and labels\n    X = dataframe.values[:, :-1]\n    y = dataframe.values[:, -1]\n    print(\"Features and labels extracted.\")\n\n    # Normalize features\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X)\n    print(\"Feature normalization done.\")\n\n    # Encode labels\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    print(\"Labels encoded.\")\n\n    # Define 5-fold cross-validation\n    num_folds = 5\n    class_accuracies = np.zeros((len(label_encoder.classes_), num_folds))\n    class_precisions = np.zeros((len(label_encoder.classes_), num_folds))\n    class_recalls = np.zeros((len(label_encoder.classes_), num_folds))\n    class_f1_scores = np.zeros((len(label_encoder.classes_), num_folds))\n\n    TPs = np.zeros((len(label_encoder.classes_), num_folds))\n    TNs = np.zeros((len(label_encoder.classes_), num_folds))\n    FPs = np.zeros((len(label_encoder.classes_), num_folds))\n    FNs = np.zeros((len(label_encoder.classes_), num_folds))\n\n    fold_accuracies = []\n    fold_cms = []\n    classifiers = []\n    print(\"Initialized cross-validation metrics.\")\n\n    for fold in range(num_folds):\n        print(f\"\\nStarting Fold {fold + 1} ...\")\n        \n        # Initialize classifier\n        classifier = XGBClassifier(max_depth=35, random_state=0)\n        print(\"Classifier initialized.\")\n\n        X_train, y_train, X_test, y_test = [], [], [], []\n        for camera_id in np.unique(X[:, -1]):\n            indices = np.where(X[:, -1] == camera_id)[0]\n            x_camera = X[indices][:, :-1]\n            y_camera = y[indices]\n\n            for label in np.unique(y_camera):\n                indices = np.where(y_camera == label)[0]\n                x_class = x_camera[indices]\n                y_class = y_camera[indices]\n\n                fold_size = int(len(indices) * 0.2)\n                start = fold * fold_size\n                end = start + fold_size\n\n                train_idxs = np.setdiff1d(np.arange(len(indices)), np.arange(start, end))\n                test_idxs = np.arange(start, end)\n\n                X_train.append(x_class[train_idxs])\n                y_train.append(y_class[train_idxs])\n                X_test.append(x_class[test_idxs])\n                y_test.append(y_class[test_idxs])\n        \n        X_train = np.concatenate(X_train, axis=0)\n        y_train = np.concatenate(y_train, axis=0)\n        print(\"Training and testing data prepared.\")\n\n        classifier.fit(X_train, y_train)\n        print(\"Classifier trained.\")\n\n        # Test the classifier\n        X_test = np.concatenate(X_test, axis=0)\n        y_test = np.concatenate(y_test, axis=0)\n        y_pred = classifier.predict(X_test)\n        print(\"Classifier tested.\")\n\n        # Compute per-class metrics\n        for label in np.unique(y_test):\n            true_idxs = np.where(y_test == label)[0]\n            pred_idxs = np.where(y_pred == label)[0]\n            not_true_idxs = np.where(y_test != label)[0]\n            not_pred_idxs = np.where(y_pred != label)[0]\n\n            TP = len(set(true_idxs) & set(pred_idxs))\n            FP = len(set(pred_idxs) - set(true_idxs))\n            FN = len(set(true_idxs) - set(pred_idxs))\n            TN = len(set(not_true_idxs) & set(not_pred_idxs))\n\n            TPs[label][fold] = TP\n            TNs[label][fold] = TN\n            FPs[label][fold] = FP\n            FNs[label][fold] = FN\n\n            accuracy = TP / len(true_idxs) if len(true_idxs) > 0 else 0\n            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n            class_accuracies[label][fold] = accuracy\n            class_precisions[label][fold] = precision\n            class_recalls[label][fold] = recall\n            class_f1_scores[label][fold] = f1_score\n\n        print(f\"Metrics computed for Fold {fold + 1}.\")\n\n        # Compute overall accuracy for this fold\n        accuracy = accuracy_score(y_test, y_pred)\n        cm = confusion_matrix(y_test, y_pred)\n        fold_accuracies.append(accuracy)\n        fold_cms.append(cm)\n        classifiers.append(classifier)\n        print(f\"Overall accuracy computed for Fold {fold + 1}.\")\n\n        # Save results for this fold\n        data = np.stack([\n            class_accuracies[:, fold],\n            class_precisions[:, fold],\n            class_recalls[:, fold],\n            class_f1_scores[:, fold]\n        ], axis=1)\n\n        dataframe = pd.DataFrame(data, columns=[\"accuracy\", \"precision\", \"recall\", \"f1_score\"])\n        dataframe[\"classes\"] = label_encoder.classes_\n        df1 = dataframe[['classes', 'accuracy', 'precision', 'recall', 'f1_score']]\n\n        data = np.stack([\n            TPs[:, fold],\n            FNs[:, fold],\n            FPs[:, fold],\n            TNs[:, fold]\n        ], axis=1)\n\n        df2 = pd.DataFrame(data, columns=[\"TPs\", \"FNs\", \"FPs\", \"TNs\"])\n        df3 = pd.concat([df1, df2], axis=1)\n\n        fold_filename = f\"{file_name.split('.')[0]}_fold_{fold + 1}_stats.csv\"\n        df3.to_csv(os.path.join(save_folder, fold_filename), index=False)\n        print(f\"Results saved for Fold {fold + 1}.\")\n\n    # Compute and save the averaged results\n    avg_data = np.stack([\n        np.mean(class_accuracies, axis=1),\n        np.mean(class_precisions, axis=1),\n        np.mean(class_recalls, axis=1),\n        np.mean(class_f1_scores, axis=1)\n    ], axis=1)\n\n    avg_dataframe = pd.DataFrame(avg_data, columns=[\"accuracy\", \"precision\", \"recall\", \"f1_score\"])\n    avg_dataframe[\"classes\"] = label_encoder.classes_\n    df1_avg = avg_dataframe[['classes', 'accuracy', 'precision', 'recall', 'f1_score']]\n\n    avg_filename = f\"{file_name.split('.')[0]}_avg_stats.csv\"\n    df1_avg.to_csv(os.path.join(save_folder, avg_filename), index=False)\n    print(f\"Averaged results saved for {file_name}.\")\n\nprint(\"\\nAll files processed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T16:14:14.011744Z","iopub.execute_input":"2025-03-18T16:14:14.012115Z","iopub.status.idle":"2025-03-18T18:05:01.936645Z","shell.execute_reply.started":"2025-03-18T16:14:14.012069Z","shell.execute_reply":"2025-03-18T18:05:01.935453Z"}},"outputs":[{"name":"stdout","text":"\nProcessing file: ALLvs1.csv ...\nCSV file loaded successfully.\nFeatures and labels extracted.\nFeature normalization done.\nLabels encoded.\nInitialized cross-validation metrics.\n\nStarting Fold 1 ...\nClassifier initialized.\nTraining and testing data prepared.\nClassifier trained.\nClassifier tested.\nMetrics computed for Fold 1.\nOverall accuracy computed for Fold 1.\nResults saved for Fold 1.\n\nStarting Fold 2 ...\nClassifier initialized.\nTraining and testing data prepared.\nClassifier trained.\nClassifier tested.\nMetrics computed for Fold 2.\nOverall accuracy computed for Fold 2.\nResults saved for Fold 2.\n\nStarting Fold 3 ...\nClassifier initialized.\nTraining and testing data prepared.\nClassifier trained.\nClassifier tested.\nMetrics computed for Fold 3.\nOverall accuracy computed for Fold 3.\nResults saved for Fold 3.\n\nStarting Fold 4 ...\nClassifier initialized.\nTraining and testing data prepared.\nClassifier trained.\nClassifier tested.\nMetrics computed for Fold 4.\nOverall accuracy computed for Fold 4.\nResults saved for Fold 4.\n\nStarting Fold 5 ...\nClassifier initialized.\nTraining and testing data prepared.\nClassifier trained.\nClassifier tested.\nMetrics computed for Fold 5.\nOverall accuracy computed for Fold 5.\nResults saved for Fold 5.\nAveraged results saved for ALLvs1.csv.\n\nAll files processed successfully!\n","output_type":"stream"}],"execution_count":3}]}